{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe9101b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import splitfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d1d0c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 253 files [00:01, 148.01 files/s]\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../Brain_Projects/brain_tumor_dataset'\n",
    "splitfolders.ratio(data_dir, output='dataset', seed=1337, ratio=(.8, .1, .1), group_prefix=None, move=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8583a965",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 120\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c981812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a686ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 202 images belonging to 2 classes.\n",
      "Found 24 images belonging to 2 classes.\n",
      "Found 27 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir = '../Brain_Projects/dataset/train'\n",
    "test_dir = '../Brain_Projects/dataset/test'\n",
    "val_dir = '../Brain_Projects/dataset/val'\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bf22e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "Inception_V3 = hub.KerasLayer(\"https://tfhub.dev/google/imagenet/inception_v3/classification/5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30578601",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "x = Inception_V3(inputs)\n",
    "outputs = layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "model_0 = tf.keras.Model(inputs, outputs, name='Model_0_untrainable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a8a4c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_0_untrainable\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 120, 120, 3)]     0         \n",
      "_________________________________________________________________\n",
      "keras_layer (KerasLayer)     (None, 1001)              23853833  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 2004      \n",
      "=================================================================\n",
      "Total params: 23,855,837\n",
      "Trainable params: 2,004\n",
      "Non-trainable params: 23,853,833\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ee0814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71727009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7/7 [==============================] - 29s 864ms/step - loss: 4.5083 - accuracy: 0.5594 - val_loss: 3.3207 - val_accuracy: 0.6250\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 1s 205ms/step - loss: 2.0826 - accuracy: 0.6881 - val_loss: 1.6853 - val_accuracy: 0.8750\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 1.2221 - accuracy: 0.7723 - val_loss: 1.3518 - val_accuracy: 0.8750\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 1s 126ms/step - loss: 0.8408 - accuracy: 0.7970 - val_loss: 1.7237 - val_accuracy: 0.8333\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 1s 121ms/step - loss: 0.6162 - accuracy: 0.8564 - val_loss: 1.4362 - val_accuracy: 0.8333\n"
     ]
    }
   ],
   "source": [
    "model_0_history = model_0.fit(\n",
    "    train_generator,\n",
    "    epochs=5,\n",
    "    validation_data=val_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb76c7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step - loss: 1.1898 - accuracy: 0.8148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1898244619369507, 0.8148148059844971]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac469e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 178ms/step - loss: 1.4362 - accuracy: 0.8333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4361897706985474, 0.8333333134651184]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0.evaluate(val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33210667",
   "metadata": {},
   "outputs": [],
   "source": [
    "Inception_V3_custom = hub.KerasLayer(\"https://tfhub.dev/google/imagenet/inception_v3/classification/5\")\n",
    "Inception_V3_custom.trainable = True\n",
    "\n",
    "inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "x = Inception_V3_custom(inputs)\n",
    "outputs = layers.Dense(2, activation='softmax')(x)\n",
    "model_1 = tf.keras.Model(inputs, outputs, name='Model_1_trainable')\n",
    "\n",
    "model_1.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f857b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_1_trainable\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 120, 120, 3)]     0         \n",
      "_________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)   (None, 1001)              23853833  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 2004      \n",
      "=================================================================\n",
      "Total params: 23,855,837\n",
      "Trainable params: 23,821,405\n",
      "Non-trainable params: 34,432\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b32d3f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7/7 [==============================] - 23s 757ms/step - loss: 1.8301 - accuracy: 0.6287 - val_loss: 154.7309 - val_accuracy: 0.3750\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 1.3863 - accuracy: 0.8564 - val_loss: 626.2628 - val_accuracy: 0.3750\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 1.5205 - accuracy: 0.8564 - val_loss: 90.1580 - val_accuracy: 0.6250\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 2s 247ms/step - loss: 1.5532 - accuracy: 0.8119 - val_loss: 3684.5164 - val_accuracy: 0.6250\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 1.1111 - accuracy: 0.8515 - val_loss: 126725.6016 - val_accuracy: 0.6250\n"
     ]
    }
   ],
   "source": [
    "model_1_history = model_1.fit(\n",
    "    train_generator,\n",
    "    validation_data = val_generator,\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab02f0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 162ms/step - loss: 126725.6016 - accuracy: 0.6250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[126725.6015625, 0.625]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.evaluate(val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcaf6375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 200ms/step - loss: 110620.1797 - accuracy: 0.5926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[110620.1796875, 0.5925925970077515]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "09a1d612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.80297013e-06, 9.99997139e-01],\n",
       "       [9.99896288e-01, 1.03756516e-04],\n",
       "       [1.03214830e-01, 8.96785140e-01],\n",
       "       [7.15939450e-08, 9.99999881e-01],\n",
       "       [6.21098101e-01, 3.78901929e-01],\n",
       "       [5.22324741e-01, 4.77675259e-01],\n",
       "       [7.70129636e-03, 9.92298663e-01],\n",
       "       [9.99529481e-01, 4.70515050e-04],\n",
       "       [9.75353062e-01, 2.46468689e-02],\n",
       "       [5.17761568e-04, 9.99482214e-01],\n",
       "       [2.29766569e-03, 9.97702301e-01],\n",
       "       [9.87352967e-01, 1.26469908e-02],\n",
       "       [9.99630332e-01, 3.69706278e-04],\n",
       "       [4.09770869e-02, 9.59022939e-01],\n",
       "       [1.72925866e-04, 9.99827087e-01],\n",
       "       [2.61599052e-04, 9.99738395e-01],\n",
       "       [1.02306463e-01, 8.97693574e-01],\n",
       "       [1.32217934e-08, 1.00000000e+00],\n",
       "       [9.22194376e-05, 9.99907732e-01],\n",
       "       [2.22754466e-06, 9.99997735e-01],\n",
       "       [9.94641185e-01, 5.35874208e-03],\n",
       "       [1.00000000e+00, 4.91610530e-10],\n",
       "       [5.68857729e-01, 4.31142241e-01],\n",
       "       [1.07380651e-01, 8.92619312e-01],\n",
       "       [9.82120216e-01, 1.78797673e-02],\n",
       "       [5.12848697e-09, 1.00000000e+00],\n",
       "       [9.99997020e-01, 2.97540259e-06]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model-0\n",
    "test_generator.reset()\n",
    "model_0_pred_probs = model_0.predict(test_generator)\n",
    "model_0_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "92a22374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(27,), dtype=int64, numpy=\n",
       "array([1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 0, 1, 0], dtype=int64)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_preds = tf.argmax(model_0_pred_probs, axis=1)\n",
    "model_0_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f2205603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'No': 0, 'Yes': 1}\n"
     ]
    }
   ],
   "source": [
    "labels = (train_generator.class_indices)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ab7048e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'No', 1: 'Yes'}\n"
     ]
    }
   ],
   "source": [
    "labels = dict((v,k) for k,v in labels.items())\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "03be7a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model_0_preds = np.array(model_0_preds).tolist()\n",
    "new_model_0_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3dfd6fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [labels[k] for k in new_model_0_preds]\n",
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b331339f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['No\\\\3 no.jpg',\n",
       " 'No\\\\49 no.jpg',\n",
       " 'No\\\\7 no.jpg',\n",
       " 'No\\\\N1.JPG',\n",
       " 'No\\\\No15.jpg',\n",
       " 'No\\\\No19.jpg',\n",
       " 'No\\\\no 4.jpg',\n",
       " 'No\\\\no 89.jpg',\n",
       " 'No\\\\no 9.png',\n",
       " 'No\\\\no 94.jpg',\n",
       " 'No\\\\no 96.jpg',\n",
       " 'Yes\\\\Y114.JPG',\n",
       " 'Yes\\\\Y15.jpg',\n",
       " 'Yes\\\\Y166.JPG',\n",
       " 'Yes\\\\Y183.jpg',\n",
       " 'Yes\\\\Y250.jpg',\n",
       " 'Yes\\\\Y257.jpg',\n",
       " 'Yes\\\\Y3.jpg',\n",
       " 'Yes\\\\Y30.jpg',\n",
       " 'Yes\\\\Y35.jpg',\n",
       " 'Yes\\\\Y37.jpg',\n",
       " 'Yes\\\\Y39.jpg',\n",
       " 'Yes\\\\Y45.JPG',\n",
       " 'Yes\\\\Y77.jpg',\n",
       " 'Yes\\\\Y90.jpg',\n",
       " 'Yes\\\\Y92.png',\n",
       " 'Yes\\\\Y98.JPG']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = test_generator.filenames\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "38046d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No\\3 no.jpg</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No\\49 no.jpg</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No\\7 no.jpg</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No\\N1.JPG</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No\\No15.jpg</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>No\\No19.jpg</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>No\\no 4.jpg</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>No\\no 89.jpg</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>No\\no 9.png</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>No\\no 94.jpg</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>No\\no 96.jpg</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Yes\\Y114.JPG</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Yes\\Y15.jpg</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Yes\\Y166.JPG</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Yes\\Y183.jpg</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Yes\\Y250.jpg</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Yes\\Y257.jpg</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Yes\\Y3.jpg</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Yes\\Y30.jpg</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Yes\\Y35.jpg</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Yes\\Y37.jpg</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Yes\\Y39.jpg</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Yes\\Y45.JPG</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Yes\\Y77.jpg</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Yes\\Y90.jpg</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Yes\\Y92.png</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Yes\\Y98.JPG</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Filename Predictions\n",
       "0    No\\3 no.jpg         Yes\n",
       "1   No\\49 no.jpg          No\n",
       "2    No\\7 no.jpg         Yes\n",
       "3      No\\N1.JPG         Yes\n",
       "4    No\\No15.jpg          No\n",
       "5    No\\No19.jpg          No\n",
       "6    No\\no 4.jpg         Yes\n",
       "7   No\\no 89.jpg          No\n",
       "8    No\\no 9.png          No\n",
       "9   No\\no 94.jpg         Yes\n",
       "10  No\\no 96.jpg         Yes\n",
       "11  Yes\\Y114.JPG          No\n",
       "12   Yes\\Y15.jpg          No\n",
       "13  Yes\\Y166.JPG         Yes\n",
       "14  Yes\\Y183.jpg         Yes\n",
       "15  Yes\\Y250.jpg         Yes\n",
       "16  Yes\\Y257.jpg         Yes\n",
       "17    Yes\\Y3.jpg         Yes\n",
       "18   Yes\\Y30.jpg         Yes\n",
       "19   Yes\\Y35.jpg         Yes\n",
       "20   Yes\\Y37.jpg          No\n",
       "21   Yes\\Y39.jpg          No\n",
       "22   Yes\\Y45.JPG          No\n",
       "23   Yes\\Y77.jpg         Yes\n",
       "24   Yes\\Y90.jpg          No\n",
       "25   Yes\\Y92.png         Yes\n",
       "26   Yes\\Y98.JPG          No"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results=pd.DataFrame({\"Filename\":filenames,\n",
    "                      \"Predictions\":predictions})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31da3749",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
